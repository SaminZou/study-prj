```yaml
title: IO模型 + 零拷贝 
author: samin
date: 2022-02-28
```

# 网络IO

## socket

1. kernel提供的一个方法，调用成功会返回一个 `fd` （文件描述符）

1. 得到 `sfd` 之后，使用 `bind()` 方法绑定一个端口号

1. 调用 `listen()` 方法开始监听端口

1. 当有客户端请求，调用 `accept()` 返回一个 `cfd` 客户端的文件描述符

1. 通过 `read(cfd)` 方法，开启读取客户端数据的系统调用

## 网络IO的变迁

### BIO

- 最开始是 `BIO` 多线程去遍历连接请求，系统调用 `accept()` ，调用阻塞的 `read()` 方法

- 线程太多容易导致程序宕机

### NIO

> 和 Java 的 NIO（New Non-Blocking） 不一样

- 发展成为 `NIO` ，系统调用 `accept4()` 方法，修改 `flags` 入参， `read()` 变为非阻塞，解决需要创建多线程的问题

- 时间复杂度可能达到 `O(n)` ，频繁使用系统调用（假设维护了1000个客户端，有一个客户端发送了请求，需要1000次调用，只有一个是有效调用，其他999次 `用户态 -> 内核态` 的切换是浪费的）

### SELECT

- 发展为多路复用，内核提供 `select()` 方法，程序的系统调用不再使用 `read()` 修改成为 `select()` ，时间复杂度下降为**O(1)**，内核去做 `fd` 的遍历，但是内核的遍历不需要进行 `用户态 -> 内核态` 的时间消耗（减少了系统调用的次数）

- 系统内核主动遍历耗费时间

### EPOLL

- 完成socket监听后，系统调用 `epoll_create()` 方法在系统中维护一个 `fd` ，调用 `epoll_ctl()` 绑定sokcet的 `fd` ，循环调用 `epoll_wait()` ，有客户端请求后，服务端调用 `accept()` 方法，和客户端简历连接生成 `fd` ，调用 `epoll_ctl()` 绑定客户端的 `fd`
- 使用 `红黑树` 数据结构存储 `fd`

> 无论是那种IO模型，最后都是程序同步模式系统调用 `read()`

> 网卡数据进来是直接走DMA的，完成后出发内核callback，epoll事件驱动是基于这个来做的

# IO模型

## 理解

对象都是针对进程；同步异步针对结果，是否要主动去查询；阻塞非阻塞是针对进程是否同时处理别的事情。

- 同步：轮询方式，每隔一段时间看任务是否完成
- 异步：任务完成主动通知
- 阻塞：一个请求，直到结束结束阻塞状态，其他任务才能进行（任务执行慢）
- 非阻塞：多个请求分别进行任务，采用轮询方式不断访问kernel是否准备好，多任务能同时进行（CPU占用率高）

## 网络IO的模型分类

- 同步模型
    - 阻塞IO
    - 非阻塞IO
    - 多路复用IO
    - 信号驱动IO

- 异步模型
    - 异步IO（异步非阻塞IO，如JAVA NIO）

## 多路复用IO

### 类比解释

假设你在大学宿舍楼，你朋友来找你玩，SELECT版舍管阿姨会一个一个房间去询问去找，直到找到你为止；EPOLL版宿管阿姨会先记下每个人的房间号，当你朋友来的时候只需要你自己直接告诉他房间号即可。所以假设有超过10000个人要来找人，可想而知哪个的效率高。

### 市面上常见应用场景

apache早期使用select模型，nginx使用epoll模型，nginx的网络IO效率明显要高于apache

### 实现方式

都是通过设定FD，但是监听方式不同。
- Select使用一个FD_SET存储监听事件，使用bitmap方式，让1024个bit分别代表不同的FD，因此有最大FD限制。
- POLL让每一个FD事件独立存储，监控所有的注册的FD。
- EPOLL基于事件驱动，有活跃可用的FD时调用callback函数。

### 主要区别

|  | select | epoll |
| ---  | ---  | ---  |
| 性能 | 随着连接数增加，性能下降，处理高并发时性能会很差 | 随着连接数增加，性能基本没影响，处理高并发时性能强 |
| 连接数 | 连接数有限制，需要设定FD_SETSIZE，重新编译 | 连接数无限制 |
| 处理机制 | 线性轮询 | 回调callback |
| 开发复杂度 | 低 | 中 |

### POLL和SELECT

没有本质上的区别，都是轮询监听的方式，只不过没有最大连接数的限制，因为采用了链表来存储FD。

### EPOLL原理

1. EPOLL和操作系统是共用一段内存存储FD，减少SELECT模式轮询查询FD所消耗的时间。
2. EPOLL是基于事件驱动，主动唤醒进程处理IO。

### EPOLL的优点

1. 没有最大并发连接的限制，能打开的FD的上限远大于1024（1G的内存上能监听约10万个端口）。
2. 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；
   即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。

### 内核态和用户态

内核态是操作系统的使用内存空间，用户态是进程的使用内存空间

## 追踪多路复用

### strace

strace可以追踪线程、进程

### 常用指令

$ strace -ff -o ~/out/threadlog ./redis-server

-ff 进程和线程都跟踪
-o 输出到文件

\# 存储了所有进程相关文件，可以进入查看

$ cd /proc/[pid]

\# 查看进程的线程文件

$ cd /proc/[pid]/task

## EPOLL实际应用

### redis

- 单线程调用 epoll

- 因为redis在内存中存数据，如果是多线程调用epoll，在操作同一个数据的时候需要考虑一致性问题，单线程即串行划解决这个问题

### nginx

- 多线程调用 epoll

- 因为最开始是web静态服务器，只是单纯读数据，没有写入操作，所以不用考虑一致性问题

# 零拷贝

## 实现方法（系统调用方法）

- mmap( )

- sendFile( )

## mmap() 和 sendFile() 的区别
- mmap 适合小数据量读写，sendFile 适合大文件传输
- mmap 需要 4 次上下文切换，3 次数据拷贝；sendFile 需要 3 次上下文切换，最少 2 次数据拷贝。
- sendFile 可以利用 DMA 方式，减少 CPU 拷贝，mmap 则不能（必须从内核拷贝到 Socket 缓冲区）。

> `RocketMQ` 在消费消息时，使用了 mmap
> `kafka` 和 `nginx` ， 使用了 sendFile
